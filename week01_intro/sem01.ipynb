{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вступление\n",
    "\n",
    "Всем привет! На сегодняшнем семинаре мы познакомимся с библиотекой **PyTorch**. Он очень похож на Numpy, с одним лишь отличием (на самом деле их больше, но сейчас мы поговорим про самое главное) -- PyTorch может считать градиенты за вас. Таким образом вам не надо будет руками писать обратный проход в нейросетях.\n",
    "\n",
    "#### Семинар построен следующим образом:\n",
    "\n",
    "1. Вспоминаем Numpy и сравниваем операции в PyTorch\n",
    "2. Создаем тензоры в PyTorch\n",
    "3. Работаем с градиентами руками\n",
    "4. Моя первая нейросеть "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вспоминаем Numpy и сравниваем операции в PyTorch\n",
    "\n",
    "Мы можем создавать матрицы, перемножать их, складывать, транспонировать и в целом совершать любые матричные операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(5, 3) # создали случайную матрицу \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Проверили размеры: {a.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Добавили 5:\\n{a + 5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X*X^T:\\n{a @ a.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Среднее по колонкам:\\n{a.mean(axis=-1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Изменили размеры: {a.reshape(3, 5).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разминка.\n",
    "\n",
    "При помощи numpy посчитайте сумму квадратов натуральных чисел от 1 до 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичные операции в **PyTorch** выглядят следующим образом, синтаксис почти не отличается:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Проверили размеры: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Добавили 5:\\n{x + 5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X*X^T:\\n{x @ x.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Среднее по колонкам:\\n{x.mean(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Изменили размеры:\\n{x.reshape([3, 5]).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Небольшой пример того, как меняются операции:\n",
    "\n",
    "* `x.sum(axis=-1) -> x.sum(dim=-1)`\n",
    "* `x.astype(np.int64) -> x.type(torch.int64)`\n",
    "\n",
    "Для помощи вам есть [таблица](https://pytorch-for-numpy-users.wkentaro.com/), которая поможет вам найти аналог операции в Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем тензоры в PyTorch и снова изучаем базовые операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(5, 3)  # пустой тензор (т.е. без инициализации)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)  # тензор со случайными числами\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.int64)  # тензор с нулями и указанием типов чисел\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([5.5, 3])  # конструируем тензор из питоновского листа\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.float64)  # используем уже созданный тензор для создания тензора из единичек\n",
    "print(x, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float32)  # создаем матрицу с размерами как у x\n",
    "print(x, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)  # операция сложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.add(x, y)  # очередная операция сложения\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.add(x, y, out=z)  # и наконец последний вид\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.zero_()  # зануление значений тензора\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x * y)  # поэлементное умножение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.mm(y.T))  # матричное умножение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x @ y.T)  # и опять матричное умножение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.unsqueeze(0).shape)  # добавили измерение в начало, аналог броадкастинга "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.unsqueeze(0).squeeze(0).shape)  # убрали измерение в начале, аналог броадкастинга "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также можем делать обычные срезы и переводить матрицы назад в numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((3, 5))\n",
    "x = torch.ones((3, 5))\n",
    "print(np.allclose(x.numpy(), a))\n",
    "print(np.allclose(x.numpy()[:, 1], a[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работаем с градиентами руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "plt.scatter(boston.data[:, -1], boston.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В pytorch есть возможность при создании тензора указывать нужно ли считать по нему градиент или нет, с помощью параметра `requires_grad`. Когда `requires_grad=True` мы сообщаем фреймворку, о том, что мы хотим следить за всеми тензорами, которые получаются из созданного. Иными словами, у любого тензора, у которого указан данный параметр, будет доступ к цепочке операций и преобразований совершенными с ними. Если эти функции дифференцируемые, то у тензора появляется параметр `.grad`, в котором хранится значение градиента.\n",
    "\n",
    "Если к тензору, получающемуся в результате, применить метод `.backward()`, то фреймворк посчитает по цепочке градиенту для всех тензоров, у которых `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "x = torch.tensor(boston.data[:, -1] / boston.data[:, -1].max(), dtype=torch.float32)\n",
    "y = torch.tensor(boston.target, dtype=torch.float32)\n",
    "\n",
    "assert w.grad is None # только создали тензоры и в них нет градиентов\n",
    "assert b.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w * x + b # и опять совершаем операции с тензорами\n",
    "loss = torch.mean((y_pred - y)**2) # совершаем операции с тензорами\n",
    "loss.backward() # считаем градиенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert w.grad is not None  # сделали операции и посчитали градиенты, значение должно было появиться\n",
    "assert b.grad is not None\n",
    "\n",
    "assert isinstance(w.grad, torch.Tensor)  # градиент — это тоже тензор\n",
    "assert isinstance(b.grad, torch.Tensor)\n",
    "\n",
    "print(\"dL/dw = \\n\", w.grad)\n",
    "print(\"dL/db = \\n\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "num_iters = 100\n",
    "\n",
    "for i in range(num_iters):\n",
    "    y_pred = w * x + b\n",
    "    # попробуйте сделать полиномиальную регрессию в данном предсказании и посчитать градиенты после\n",
    "    loss = torch.mean((y_pred - y)**2)\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # делаем шаг градиентного спуска с lr = .05\n",
    "        w -= <YOUR CODE>\n",
    "        b -= <YOUR CODE>\n",
    "\n",
    "        # обнуляем градиенты, чтобы на следующем шаге опять посчитать и не аккумулировать их\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "    # рисуем картинки\n",
    "    if (i + 1) % 5 == 0:\n",
    "        clear_output(True)\n",
    "        plt.scatter(x.numpy(), y.numpy())\n",
    "        # PyTorch запрещает вызывать .numpy() на тензорах, у которых requires_grad=True, поэтому\n",
    "        # вначале делаем копию тензора при помощи .detach()\n",
    "        plt.scatter(x.numpy(), y_pred.detach().numpy(), color='orange', linewidth=5)\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"[Iteration {i}] loss = {loss.detach().numpy()}\")\n",
    "        if loss.detach().numpy() < 0.5:\n",
    "            print(\"Done!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Моя первая нейросеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы разобраться как обучать нейросети в PyTorch, нужно освоить три вещи: \n",
    "\n",
    "1. Как формировать батчи и пихать их в сетку\n",
    "2. Как сделать сетку\n",
    "3. Как написать цикл обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Как формировать батчи и пихать их в сетку\n",
    "\n",
    "Чтобы в данном фреймворке иметь возможность итерироваться по данным и применять к ним преобразования, например, аугментации, о которых вы узнаете позже -- нужно создать свой класс унаследованный от `torch.utils.data.Dataset`.\n",
    "\n",
    "Вот пример из документации:\n",
    "\n",
    "```\n",
    "class FaceLandmarksDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "```\n",
    "\n",
    "Как вы видите, у такого класса должно быть два метода: \n",
    "\n",
    "* `__len__`: возвращает информацию о том, сколько объектов у нас в датасете\n",
    "* `__getitem__`: возвращает семпл и таргет к нему\n",
    "\n",
    "\n",
    "Теперь давайте напишем такой сами, в качестве датасета сгенерируем рандомные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Our random dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {'sample': torch.tensor(x[idx, :], dtype=torch.float), 'target': y[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(1000, 5)\n",
    "y = np.random.rand(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_dataset = RandomDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_dataset[1]  # [1] под капотом вызывает .__getitem__(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы из данных получать батчи в pytorch используется такая сущность как даталоадер, который принимает на вход класс унаследованный от `torch.utils.data.Dataset`. Сейчас посмотрим на пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(our_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работают с ним следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    batch_x = batch['sample']\n",
    "    batch_y = batch['target']\n",
    "    print('Sample:', batch_x)\n",
    "    print('Target:', batch_y)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Как сделать сетку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы в high-level pytorch создавать нейросети используется модуль `nn`. Нейросеть должна быть унаследована от класса `nn.Module`. Пример как это может выглядеть:\n",
    "\n",
    "```\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "       x = F.relu(self.conv1(x))\n",
    "       return F.relu(self.conv2(x))\n",
    "```\n",
    "\n",
    "Как мы видим на данном примере, у данного класса должно быть метод `forward`, который определяет прямой проход нейросети. Также из класса выше видно, что модуль `nn` содержит в себе реализацию большинства слоев, а модуль `nn.functional` -- функций активаций.\n",
    "\n",
    "Есть еще один способ создать нейросеть и давайте его разберем на практике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(  # создаем контейнер, который инициализируем списком слоёв\n",
    "    nn.Linear(5, 3),    # добавили слой с 5-ю нейронами на вход и 3-мя на выход\n",
    "    nn.ReLU(),          # добавили функцию активации\n",
    "    nn.Linear(3, 1),    # добавили слой с 3-мя нейронами на вход и 5-ю на выход\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(batch_x) # получили предсказания модели\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на `grad_fn`. Тензор `y_pred` помнит про то, что последней операцией в его вычислительном графе была [`addmm`](https://pytorch.org/docs/stable/generated/torch.addmm.html), то есть (упрощая) `b + m @ x`. Это соответствует `nn.Linear` в конце модели!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Как написать цикл обучения\n",
    " \n",
    "Давайте теперь соберем теперь загрузку данных, создание модели и обучим на уже созданном для нас датасете MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтобы сайт, на котором выложен датасет, не принял нас за ботов, прикинемся браузером\n",
    "\n",
    "import urllib\n",
    "\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.hub import _get_torch_home\n",
    "\n",
    "# На Linux датасет скачается в ~/.cache/torch/datasets, но можете выбрать любую другую папку\n",
    "mnist_path = Path(_get_torch_home()) / 'datasets'\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    mnist_path, train=True, download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ") # используем готовый класс от торча для загрузки данных для тренировки\n",
    "mnist_valid = torchvision.datasets.MNIST(\n",
    "    mnist_path, train=False, download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ") # используем готовый класс от торча для загрузки данных для валидации\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    mnist_train, batch_size=4, shuffle=True, num_workers=1\n",
    ") # так как это уже унаследованный от Dataset класс, его можно сразу пихать в даталоадер\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    mnist_valid, batch_size=4, shuffle=False, num_workers=1\n",
    ") # так как это уже унаследованный от Dataset класс, его можно сразу пихать в даталоадер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0, 1]:\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.imshow(mnist_train[i][0].squeeze(0).numpy().reshape([28, 28]), cmap='gray')\n",
    "    plt.title(str(mnist_train[i][1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Flatten(),         # превращаем картинку 28х28 в вектор размером 784\n",
    "    nn.Linear(784, 128),  # входной слой размером 784 нейронов с выходом в 128 нейронов\n",
    "    nn.ReLU(),            # функция активации релу\n",
    "    nn.Linear(128, 10),   # ещё один линейный слой\n",
    ")\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.05) # создаем оптимизатор и передаем туда параметры модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрите внимательно: мы сейчас будем заниматься классификацией, но в конце модели нет никакого софтмакса! Как так?\n",
    "\n",
    "Дело в том, что поскольку во время обучения мы будем оптимизировать negative log likelihood, после softmax в функции потерь будет сразу стоять логарифм. Последовательное вычисление сначала softmax, а потом его логарифма может приводить к большим ошибкам округления, поэтому обычно эти две функции соединяют в композицию `log_softmax`, которая ведёт себя гораздо лучше:\n",
    "\n",
    "$$\n",
    "\\log \\left[ \\operatorname{softmax}(x) \\right]_i =\n",
    "\\log \\left( \\frac {\\exp(x_i)} {\\sum_{j=1}^n \\exp(x_j)} \\right) =\n",
    "x_i - \\log\\left( \\sum_{j=1}^n \\exp(x_j) \\right) =\n",
    "x_i - \\log\\left( \\sum_{j=1}^n \\exp(x_j - x_* + x_*) \\right) =\n",
    "x_i - x_* - \\log\\left( \\sum_{j=1}^n \\exp(x_j - x_*) \\right),\n",
    "$$\n",
    "\n",
    "где $x_* = \\max \\left\\{ x_i \\right\\}$.\n",
    "\n",
    "Поэтому мы можем:\n",
    "\n",
    "* Либо поставить на выход модели функцию активации `log_softmax` и учить её с функцией потерь negative log likelihood (в PyTorch она называется `nll_loss`),\n",
    "* Либо оставить модель безо всякой функции активации в конце и учить её с функцией потерь `nll_loss(log_softmax())`. В PyTorch такая композитная функция потерь называется `cross_entropy`, ей мы и воспользуемся.\n",
    "\n",
    "А когда мы захотим предсказать классы, будет достаточно просто посчитать `argmax(model(), dim=-1).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Веса моделей хранятся в виде матриц и выглядят так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in model.named_parameters()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 11):  # всего у нас будет 10 эпох (10 раз подряд пройдемся по всем батчам из трейна)\n",
    "    # Трейн\n",
    "    for x_train, y_train in tqdm(train_dataloader, desc=f'Epoch {epoch} | Train'):\n",
    "        y_pred = model(x_train) # делаем предсказания\n",
    "        loss = F.cross_entropy(y_pred, y_train) # считаем лосс\n",
    "        \n",
    "        ############################## Собственно обучение ##############################\n",
    "        # 1. Обнуляем посчитанные градиенты параметров. Забыть про это — частая ошибка!\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # 2. Считаем градиенты обратным проходом\n",
    "        loss.backward()\n",
    "        \n",
    "        # 3. Обновляем параметры сети\n",
    "        opt.step()\n",
    "        #################################################################################\n",
    "\n",
    "    # Валидация на каждой второй эпохе\n",
    "    if epoch % 2 == 0:\n",
    "        mean_valid_loss = [] # сюда будем складывать средний лосс по батчам\n",
    "        valid_accuracy = []\n",
    "        # мы считаем качество, поэтому мы запрещаем фреймворку считать градиенты по параметрам\n",
    "        with torch.no_grad():\n",
    "            for x_valid, y_valid in tqdm(valid_dataloader, desc=f'Epoch {epoch} | Valid'):\n",
    "                y_pred = model(x_valid) # делаем предсказания\n",
    "                loss = F.cross_entropy(y_pred, y_valid) # считаем лосс\n",
    "                mean_valid_loss.append(loss.item()) # добавляем в массив\n",
    "                valid_accuracy.extend((torch.argmax(y_pred, dim=-1) == y_valid).numpy().tolist())\n",
    "\n",
    "        # выводим статистику\n",
    "        print(f'Epoch: {epoch}, loss: {np.mean(mean_valid_loss):.5f}, accuracy: {np.mean(valid_accuracy)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительные материалы:\n",
    "\n",
    "* [PyTorch за 60 минут](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "* [Использование PyTorch на GPU](https://pytorch.org/docs/master/notes/cuda.html)\n",
    "* [Хорошая книга про PyTorch](https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits\n",
    "\n",
    "Этот ноутбук основан на [ноутбуке](https://github.com/hse-ds/iad-deep-learning/blob/86313e3/sem01/sem01.ipynb) первого семинара курса по ИДА в Вышке, который, в свою очередь, основан на вводном [ноутбуке](https://github.com/yandexdataschool/Practical_DL/blob/fall20/week02_autodiff/seminar_pytorch.ipynb) второй недели курса по Deep Learning в ШАДе."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
